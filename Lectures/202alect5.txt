1. Admin things.2. Kernel density estimation.3. 2-d kernel smoothing.4. Simulating from a density by rejection sampling.5. Maps.6. R Cookbook ch. 8-9.1. Admin things.You must work independently for all 4 homeworks!Read through ch10 for next class.Attendance will be mandatory the last 3 classes of the quarter.2. Kernel density estimation.   x = rnorm(1000)   hist(x,nclass=40,prob=T,main="simulated N(0,1) random variables")   lines(density(x),lty=2,col="red")density(), described on p250, is useful for kernel density estimation.Suppose your univariate data are x1, x2, ..., xn.For any number x0, and any h,f^(x0) = 1/(hn) · k((x0-x_i / h), where the sum is from 1 to n,and where k() is some density, usually centered at 0,and usually with a peak at 0.By f^, I mean fhat, our estimate of the true density, f.Here f is normal.h is called the bandwidth. If h is small, then each pointgives a lot of density nearby,whereas if h is large, then each point gets smoothed out heavily.People have shown that the kernel, k, doesn't matter nearly as muchas the choice of bandwidth.See help(density).By default, kernel = "gaussian".By default, bw="nrd0", which is Silverman(1986)'s rule of thumb,0.9 s n^(-.2) .Here, s is an estimate of sd, given by min{sample sd, IQR/1.34}.This is often too small, so Scott(1992) recommended 1.06 s n^(-.2) .bw.nrd0() yields Silverman's rule, and bw.nrd() yield's Scott's.   bw.nrd0(x)   0.9 * min(sd(x),IQR(x)/1.34) * 1000^(-.2)   bw.nrd(x)   1.06 * min(sd(x),IQR(x)/1.34) * 1000^(-.2)##### PLOTTING DENSITY ESTIMATES WITH A LEGEND   hist(x,nclass=40,prob=T,main="simulated N(0,1) random variables")   b2 = bw.nrd(x)   lines(density(x,bw=b2),col="red")   lines(density(x,bw=10*b2),col="blue")   lines(density(x,bw=.1*b2),col="brown")   lines(sort(x),dnorm(sort(x)),col="green")   legend("topright",c("Scott's bw", "huge bw", "tiny bw", "truth"),      lty=1,col=c("red","blue","brown","green"),cex=0.8)To compute kernel estimates on a grid, supply density() with n, from, and to.Or do it yourself.Let's take a really small example.    x = c(1,2.8,3)  z = density(x,bw=0.08,n=101,from=0,to=4) ## by default, n = 512 and it goes from min(x) - 3bw to max(x) + 3bw.  plot(c(0,4),c(0,2),type="n",xlab="x",ylab="density",yaxs="i")  lines(z,col="red")  Note that the curve looks smooth but it's really discrete.  z$x[1:10]  points(z$x, z$y)Consider, for instance, the kernel density estimate f^(x0) at x0 = 1.20.  z$x[31] ## 1.20  z$y[31] ## f^(1.2) = 0.07465388.But density() does some slightly weird things with small datasets especially.In help(density),"The algorithm used in density.default disperses the mass of the empirical distribution function over a regular grid of at least 512 points and then uses the fast Fourier transform to convolve this approximation with a discretized version of the kernel and then uses linear approximationto evaluate the density at the specified points."Alternatively, you can find the kernel density estimate yourself.    density2 = function(x1,xgrid,bw2){    ## x1 = data.    ## xgrid = vector of values where you'll compute the kernel estimate.    ## bw2 = bandwidth       n = length(xgrid)       y = rep(0, n)       for(i in 1:n){         y[i] = sum(dnorm(x1-xgrid[i], sd=bw2)) / length(x1)	 }        y    }  x = c(1,2.8,3)  g = seq(0,4,length=101)  y = density2(x,g,0.08)  y[31] ## 0.07303459  plot(g,y,type="l",xlab="x",ylab="density")  Look at the difference between density() and density2().    plot(g,z$y - y)You can make the grid and bandwidth whatever you want.  x1 = c(1,2.8,3)  grid1 = seq(-2,6,length=101)  bw1 = bw.nrd(x1)  y = density2(x1,grid1,bw1)  plot(grid1,y,type="l",xlab="x",ylab="density")3. 2-d kernel smoothing, using kernel2d() in splancs.     install.packages("splancs")  library(splancs)## First, input 23 points using the mouse.  n = 23  plot(c(0,1),c(0,1),type="n",xlab="longitude",ylab="latitude",main="locations")  x1 = rep(0,n)  y1 = rep(0,n)  for(i in 1:n){  z1 = locator(1)  x1[i] = z1$x  y1[i] = z1$y  points(x1[i],y1[i])  }##### PLOT THE POINTS WITH A 2D KERNEL SMOOTHING IN GREYSCALE PLUS A LEGEND  bdw = sqrt(bw.nrd0(x1)^2+bw.nrd0(y1)^2)  ## a possible default bandwidth  b1 = as.points(x1,y1)  bdry = matrix(c(0,0,1,0,1,1,0,1,0,0),ncol=2,byrow=T)  z = kernel2d(b1,bdry,bdw)  attributes(z)  par(mfrow=c(1,2))  image(z,col=gray((64:20)/64),xlab="km E of origin",ylab="km N of origin")  points(b1)  x4 = seq(min(z$z),max(z$z),length=100)  plot(c(0,10),c(.8*min(x4),1.2*max(x4)),type="n",axes=F,xlab="",ylab="")  image(c(-1:1),x4,matrix(rep(x4,2),ncol=100,byrow=T),add=T,col=gray((64:20)/64))  text(2,min(x4),as.character(signif(min(x4),2)),cex=1)  text(2,(max(x4)+min(x4))/2,as.character(signif((max(x4)+min(x4))/2,2)),cex=1)  text(2,max(x4),as.character(signif(max(x4),2)),cex=1)  mtext(s=3,l=-3,at=1,"density (pts/km^2)")4. Simulating from a density by rejection sampling.To do simulation by rejection sampling, you need the density f(x) you're interested in to have the property f(x) <= b g(x), where b is some constant and g(x) is some density that's easy to sample from. For instance, if f(x) is known to have some known maximum c, and finite support of length d, then you can use g(x) is the uniform distribution, and let b = cd.Basically, the way it works is you simulate x0 from g(x), keep it with probability f(x0)/[bg(x0)] and delete it otherwise, and repeat.The idea is this. Sample with density g(x), and thenkeep each point with prob. f(x)/[bg(x)].This makes sense since f(x) is always ² bg(x).The resulting density is thus g(x) f(x) / [bg(x)]= f(x)/b.So we're keeping point x with prob. density f(x)/b.But since b is constant, if you consider the density of KEPT pts,it's simply f(x).First, for simplicity, suppose the density has a nice closed form, like the triangular density on x in (0,2), f(x) = 1 - |x-1|,Here c = 1, and the support is (0,2), so d = 2, and b = cd = 2.## Plot the density  x = seq(-1,3,length=101)  y = rep(0,101)  y[(x>0) & (x<2)] = 1 - abs(x[(x>0) & (x<2)] - 1)  plot(x,y,type="l",ylab="f(x)",lty=2)To simulate from this density using rejection sampling  a) Simulate a draw x0 from g(x) = uniform on (0,2) = 1/2 on (0,2).  b) Keep x0 with probability f(x0) / [bg(x0)] .  c) Repeat.## 10000 draws from the triangular f(x)  b = 2; g = 0.5  n = 10000  x = c() ## these will contain the results.  i = 0  while(i < n){  x0 = runif(1)*2  fx0 = 1 - abs(x0 - 1)  if(runif(1) < fx0 / (b*g)){ ## keep x0    i = i+1    x[i] = x0    if(i/100 == floor(i/100)) cat(i, " ")    }  }  hist(x,nclass=40,prob=T,add=T,col="green")What if you want to sample from a more complicated density f(x), such as a kernel smoothing? Again, let c = the maximum of f(x), which you can find by looking at a plot of f(x). The support might not be finite, technically, but you can often find a finite region that effectively contains almost all the support of f(x).Let's use the simple example from before where we smoothed 3 points at 1, 2.8, and 3.   x1 = c(1,2.8,3)  grid1 = seq(-2,6,length=101)  bw1 = bw.nrd(x1)  y = density2(x1,grid1,bw1)  plot(grid1,y,type="l",xlab="x",ylab="density")Suppose we want to simulate 30,000 draws from this density f(x).The density clearly does not go higher than 0.5 anywhere, and (-2,6) is plenty wide to contain almost all the support of f(x). So, c = 0.5, d = 8, b = cd = 4.The only hard part is that f(x0) now needs to be computed for each x0.  b = 4; g = 1/8  n = 30000  x = c() ## these will contain the results.  i = 0  while(i < n){  x0 = runif(1)*8 - 2 ## this simulates the uniform g(x) on (-2,6)  fx0 = density2(x1,x0,bw1) ## this computes f(x0)   if(runif(1) < fx0 / (b*g)){ ## keep x0    i = i+1    x[i] = x0    if(i/100 == floor(i/100)) cat(i, " ")    }  }  hist(x,nclass=40,prob=T,add=T,col="green")5. Maps.Map of California counties.  install.packages("maps")  library(maps)  map('county', 'California')  plot(seq(-118,-116,length=10),seq(33,35,length=10))  map('county', 'California', add=T) ## It's not easy to make it look good.6. R Cookbook ch8.dnorm(), pnorm(),  qnorm(),                 rnorm() are the normal pdf,     cdf,      quantile function, and simulator functions, respectively.    dnorm(1.2) ## normal density  1/(sqrt(2*pi))*exp(-1.2^2/2)  pnorm(1.96) ## normal cdf  pnorm(0)  qnorm(.5) ## normal quantile  qnorm(0.975) ## see p190  qnorm(0)  qnorm(1)  qnorm(1.1) ## error  rnorm(3) ## pseudo-random normalsThe default is mean = 0, sd = 1, but you can set these.  rnorm(3, mean = 50, sd = 10)  qnorm(0.975, mean = 50, sd = 10)  rnorm(3, 5, 40) ## mean = 5 and sd = 40  rnorm(3, 5, mean=40) ## mean = 40 and sd = 5You can substitute "norm" for "binom", "geom", etc. See tables on pp177-178. We've seen "unif" already. See the warning about rexp() on p178. The parameter is the rate 1/beta, not the mean beta.  rexp(3, 10)  rexp(3, 0.1)The arguments can be vectors instead of scalars, and R will cyclethrough them one at a time, as described on p182.  rexp(3, c(.01, 10, .02))  signif(rexp(3, c(.01, 10, .02)),3)  signif(rexp(3, c(.01, 10)) ,2) ## it cycles through, so the 3rd has mean 100.The note about ?Normal and ?TDist on pp 178-179 is silly. You can use help() with a function as usual. For instance, functions for simulating normal or t distributed random variables are rnorm() and rt(), and you can do help(rnorm) or help(rt). You just can't do help(norm) or help(t) to get these help files, because norm() and t() are matrix operator functions [norm and transpose, respectively].p179, choose(n,k) gives the number of combinations. p180, combn(1:5,3) or combn(5,3) gives the actual combinations. See for instance  combn(4,3)  combn(c(8,-5,40,20),2)  combn(c(8,8,40,20),2)p184, sample(1:10, 5) chooses 5 items randomly from the vector (1:10).    sample(1:10,5)By default it is without replacement, but you can do    sample(1:10,5, rep=T)  sample(1:10,c(5,4,2)) ## doesn't do what you'd think. Only the 5 is read.If you want to generate 3 samples of different sizes from the vector 1:10, you could do  y = c(5,4,2)  x = list()  for(i in 1:3){    x[[i]] = sample(1:10,y[i])  }You can also have sample choose elements with different probabilities, by giving it as an argument a vector of probabilities, as on p185.    sample(1:10, 50, rep=T, prob = (1:10)/55)p187, diff() calculates the differences between successive elements of a vector. It returns a vector of length n-1, if your initial vector had length n. Note that when your vector is a list of times, often you want to sort your vector before using diff().    t = rpois(10, lambda=20)  diff(t)  t1 = sort(t)  t1  diff(t1)pp 191-192 show an interesting example of plotting densities. He calculates the density on a vector x of points all at once.    dnorm((1:10)/10)We can do this with the exponential distribution.    x = seq(0,10,length.out=100)  y = dexp(x)  plot(x,y, main="Exponential distribution", type="l",    ylab="density", xlab="value")Note that he uses the polygon() function to shade in a region. region.x consists of all the x-coordinates of the region.region.y consists of all the y-coordinates of the region.    x1 = x[1 <= x & x <= 3]  region.x = c(min(x1), x1, max(x1)) ## Teetor uses x1[1] and tail(x1,1) instead of min() and max()   y1 = y[1 <= x & x <= 3]  region.y = c(0,y1,0)  polygon(region.x, region.y, col="blue", density = 20)p199, mean(x > 0) computes the relative frequency, because (x > 0) is a vector of TRUEs (1) and FALSEs (0).    y = runif(50000)  mean(y > 0.8)p200 describes table(), which we've seen before. p201, summary(table) does a chi square test forindependence between 2 factors. A small p-value indicates statistically significant dependence, i.e. strong evidence against the assumption of independence.  shy = factor(c(rep("Y", 23), rep("N",20)))  gpa = factor(c(rep("A",10), rep("B", 10), rep("C",10), rep("A", 2),      rep("C", 2), rep("B", 9)))  table(shy)  table(gpa)  table(shy,gpa)  summary(table(shy, gpa))p203, scale() normalizes a numerical vector, matrix, or data frame.    x = c(1,2,4,4,3)  y = scale(x)  y[1:5]p203-204, t.test() to test against a null hypothesis of the mean, or to get a confidence interval for the population mean.    t.test(x, mu=1)  t.test(x, conf.level = 0.975)  y = t.test(x, conf.level = 0.975)  attributes(y)  y$conf.int  y$conf.int[1:2]p206, wilcox.test(x, conf.int=TRUE) ## CI for the median    x = c(1:10,-7.1,20)  wilcox.test(x, conf.int=TRUE)p207, testing if the population proportion of successes = some null hypothesis value. Say you have 70 successes out of 97 trials, and your null hypothesis is that p = 0.80.    prop.test(70,97,0.8)You can use this too to get a CI for p.  prop.test(70,97,0.8, conf.level = 0.9)Test for normality.  shapiro.test(x)p210, in the nortest package are more tests for normality.p211, testing if the total number of runs, or streaks, or equivalently changes, in binary time series data, is significantly different from what you'd expect by chanceif the observations are iid,using runs.test().    install.packages("tseries")  library(tseries)  x = runif(50)  y = (x > 0.4)  runs.test(as.factor(y))    x = sample(c(0,1,2),50,rep=T)  runs.test(as.factor(x)) ## error because the data are not binary.p212, t.test() to test if the difference between two sample means is statistically significant.    x = runif(400)  y = rnorm(70000, mean=.57)  t.test(x,y)By default, paired = FALSE, but if you have paired data andx and y have the same length you can say  t.test(x,y[1:400],paired = TRUE)p214, an alternative is the nonparametric Wilcoxon-Mann-Whitney test,where you calculate the sum of the ranks of values of x in the vector c(x,y), and compare with what you'd expect if the two distributions were equivalent.  wilcox.test(x,y)p215, Pearson test to see if a correlation is statistically significant.x and y must have the same length.  x = runif(200)  y = 0.8*x + rnorm(200)  plot(x,y,main=expression(x[i]^-2))  cor.test(x,y)  abline(lsfit(x,y))Equivalently, you can fit a line by regression and use the t-test on the coefficient for y.    b = lm(y ~ x)  ## similar to b = lsfit(x,y)  summary(b)  abline(b)  attributes(b) They're the same.    summary(b)$coef[8]  cor.test(x,y)$p.valueThe book also discusses prop.test() on p217, for comparing two proportions,pairwise.t.test() on p218, for comparing the means of > 2 samples,and the Kolmogorov-Smirnov test ks.test() to see if two cumulative distribution functions might be equivalent.